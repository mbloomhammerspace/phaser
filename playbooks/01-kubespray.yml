---
- name: Deploy Kubernetes cluster with Kubespray
  hosts: k8s_cluster
  become: true
  gather_facts: true
  
  vars:
    kubespray_version: "v2.26.0"
    kubernetes_version: "v1.31.0"
    container_manager: "containerd"
    network_plugin: "calico"
    
  pre_tasks:
    - name: Update package cache
      apt:
        update_cache: yes
        cache_valid_time: 3600
      when: ansible_os_family == "Debian"
      
    - name: Install required packages
      apt:
        name:
          - python3
          - python3-pip
          - python3-venv
          - python3-full
          - git
          - curl
          - wget
          - unzip
        state: present
      when: ansible_os_family == "Debian"
      
    - name: Install required packages (RHEL/CentOS)
      yum:
        name:
          - python3
          - python3-pip
          - git
          - curl
          - wget
          - unzip
        state: present
      when: ansible_os_family == "RedHat"
      
  tasks:
    - name: Create kubespray directory
      file:
        path: /opt/kubespray
        state: directory
        mode: '0755'
        
    - name: Clone Kubespray repository
      git:
        repo: "https://github.com/kubernetes-sigs/kubespray.git"
        dest: /opt/kubespray
        version: "{{ kubespray_version }}"
        
    - name: Create Python virtual environment for kubespray
      command: python3 -m venv /opt/kubespray/venv
      args:
        creates: /opt/kubespray/venv
        
    - name: Install Python dependencies in virtual environment
      pip:
        requirements: /opt/kubespray/requirements.txt
        virtualenv: /opt/kubespray/venv
        state: present
        
    - name: Create kubespray inventory file
      copy:
        content: |
          all:
            hosts:
          {% for host in groups['kube_control_plane'] %}
              {{ host }}:
                ansible_host: {{ hostvars[host]['ansible_host'] }}
                ansible_user: {{ hostvars[host]['ansible_user'] }}
                ansible_ssh_private_key_file: /home/ubuntu/.ssh/id_ed25519
                ansible_ssh_common_args: {{ hostvars[host]['ansible_ssh_common_args'] }}
                ip: {{ hostvars[host]['ansible_host'] }}
                access_ip: {{ hostvars[host]['ansible_host'] }}
          {% endfor %}
          {% for host in groups['kube_node'] %}
              {{ host }}:
                ansible_host: {{ hostvars[host]['ansible_host'] }}
                ansible_user: {{ hostvars[host]['ansible_user'] }}
                ansible_ssh_private_key_file: /home/ubuntu/.ssh/id_ed25519
                ansible_ssh_common_args: {{ hostvars[host]['ansible_ssh_common_args'] }}
                ip: {{ hostvars[host]['ansible_host'] }}
                access_ip: {{ hostvars[host]['ansible_host'] }}
          {% endfor %}
            children:
              kube_control_plane:
                hosts:
          {% for host in groups['kube_control_plane'] %}
                  {{ host }}:
          {% endfor %}
              kube_node:
                hosts:
          {% for host in groups['kube_node'] %}
                  {{ host }}:
          {% endfor %}
              k8s_cluster:
                children:
                  kube_control_plane:
                  kube_node:
              calico_rr:
                hosts: {}
        dest: /opt/kubespray/inventory.yml
        mode: '0644'
        
    - name: Copy SSH key from local to remote hosts
      copy:
        src: ~/.ssh/id_ed25519
        dest: /home/ubuntu/.ssh/id_ed25519
        mode: '0600'
        owner: ubuntu
        group: ubuntu
        
    - name: Copy SSH key to root user
      copy:
        src: /home/ubuntu/.ssh/id_ed25519
        dest: /root/.ssh/id_ed25519
        mode: '0600'
        owner: root
        group: root
        remote_src: yes
        
    - name: Create group_vars directory
      file:
        path: /opt/kubespray/group_vars
        state: directory
        mode: '0755'
        
    - name: Create kubespray group_vars file
      copy:
        content: |
          # Kubespray Configuration
          kube_version: v1.31.0
          container_manager: containerd
          network_plugin: calico
          dns_mode: coredns
          helm_enabled: true
          metrics_server_enabled: true
          local_path_provisioner_enabled: true

          # Container runtime
          containerd_config:
            version: 1.7.13
            systemd_cgroup: true

          # Network configuration
          calico_version: v3.26.1
          calico_ipv4pool_ipip: "CrossSubnet"
          calico_ipv4pool_vxlan: "Never"

          # Storage
          local_volume_provisioner_enabled: true
          local_volume_provisioner_base_path: /mnt/disks

          # Security
          podsecuritypolicy_enabled: false
          rbac_enabled: true

          # Monitoring
          prometheus_enabled: true
          grafana_enabled: true

          # GPU support
          nvidia_gpu_enabled: true
          nvidia_driver_enabled: true
        dest: /opt/kubespray/group_vars/all.yml
        mode: '0644'
        
    - name: Run Kubespray cluster deployment
      command: /opt/kubespray/venv/bin/ansible-playbook -i inventory.yml cluster.yml
      args:
        chdir: /opt/kubespray
      register: kubespray_result
      
    - name: Copy kubeconfig to local machine
      fetch:
        src: /etc/kubernetes/admin.conf
        dest: "{{ playbook_dir }}/kubeconfig"
        flat: yes
      when: inventory_hostname in groups['kube_control_plane']
      
    - name: Set kubectl context
      shell: |
        export KUBECONFIG={{ playbook_dir }}/kubeconfig
        /opt/kubespray/venv/bin/kubectl config set-cluster kubernetes --server=https://{{ ansible_default_ipv4.address }}:6443
        /opt/kubespray/venv/bin/kubectl config set-credentials admin --client-certificate=/etc/kubernetes/admin.conf --client-key=/etc/kubernetes/admin.key
        /opt/kubespray/venv/bin/kubectl config set-context kubernetes --cluster=kubernetes --user=admin
        /opt/kubespray/venv/bin/kubectl config use-context kubernetes
      when: inventory_hostname in groups['kube_control_plane']
      
  post_tasks:
    - name: Wait for cluster to be ready
      shell: /opt/kubespray/venv/bin/kubectl get nodes --no-headers | wc -l
      register: node_count
      until: node_count.stdout | int >= (groups['k8s_cluster'] | length)
      retries: 30
      delay: 10
      when: inventory_hostname in groups['kube_control_plane']
      
    - name: Verify cluster status
      shell: /opt/kubespray/venv/bin/kubectl get nodes -o wide
      when: inventory_hostname in groups['kube_control_plane']
